{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1cdcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–‘íŒŒ : 1201 # ë°°ì¶” : 1001 # ìƒì¶” : 1005 # ì‚¬ê³¼ : 0601 # ë¬´ : 1101 # ê°ì : 0501 # ëŒ€íŒŒ : 1202 # ê±´ê³ ì¶” : 1207\n",
    "# ë§ˆëŠ˜ : 1209 # ë”¸ê¸° : 0804  # ë°©ìš¸í† ë§ˆí†  : 0806 # ì˜¤ì´ : 0901 # ì–‘ë°°ì¶” : 1004  # ê³ êµ¬ë§ˆ : 0502  # ë°° : 0602\n",
    "\n",
    "item_nm = 'ë¬´'\n",
    "item_cd = 1101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a849626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ\n",
    "base_path = f\"{item_nm}/ìœ í†µê³µì‚¬_ë„ë§¤ì‹œì¥_{item_nm}.csv\"\n",
    "\n",
    "# retry íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘\n",
    "retry_paths = glob.glob(f\"{item_nm}/ìœ í†µê³µì‚¬_retry_{item_nm}_*.csv\")\n",
    "\n",
    "# ê¸°ë³¸ íŒŒì¼ ì½ê¸°\n",
    "df_list = [pd.read_csv(base_path, encoding=\"cp949\", low_memory=False)]\n",
    "\n",
    "# retry íŒŒì¼ë“¤ ì½ê¸° (ìˆì„ ê²½ìš°ì—ë§Œ)\n",
    "for path in retry_paths:\n",
    "    df_list.append(pd.read_csv(path, encoding=\"cp949\", low_memory=False))\n",
    "\n",
    "# ë³‘í•©\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# ì¤‘ë³µ ì œê±° (ì „ì²´ í–‰ ê¸°ì¤€ ë˜ëŠ” ì£¼ìš” ì—´ ê¸°ì¤€)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# plor_cdê°€ ë¬¸ìì—´ì´ ì•„ë‹ ê°€ëŠ¥ì„± ëŒ€ë¹„\n",
    "df['plor_cd'] = df['plor_cd'].fillna('').astype(str)\n",
    "\n",
    "pattern = r'^[^0-9]+$'\n",
    "\n",
    "condition = (\n",
    "    df['totprc'].isna() | (df['totprc'] <= 0) |\n",
    "    df['unit_tot_qty'].isna() | (df['unit_tot_qty'] <= 0) |\n",
    "    df['plor_cd'].str.strip().isin(['0', '0.0']) |\n",
    "    df['plor_cd'].str.match(pattern, na=False) |\n",
    "    df['plor_nm'].isna() |\n",
    "    (df['plor_nm'] == 0)\n",
    ")\n",
    "\n",
    "# ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” í–‰ ì¶”ì¶œ\n",
    "df_filtered = df[~condition]\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df_filtered.to_csv(f\"{item_nm}/ìœ í†µê³µì‚¬_ë„ë§¤ì‹œì¥_{item_nm}_ê²°ì¸¡ì¹˜ì œê±°.csv\", index=False, encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12c3d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "input_path = f\"{item_nm}/ìœ í†µê³µì‚¬_ë„ë§¤ì‹œì¥_{item_nm}_ê²°ì¸¡ì¹˜ì œê±°.csv\"\n",
    "output_path = f\"{item_nm}/ìœ í†µê³µì‚¬_ë„ë§¤ì‹œì¥_{item_nm}_í•œê¸€ì»¬ëŸ¼ëª….csv\"\n",
    "\n",
    "# 2. CSV ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë¶ˆëŸ¬ì˜¤ê¸° (ìµœì´ˆ ê²½ê³  ë°©ì§€)\n",
    "df = pd.read_csv(input_path, encoding='cp949', dtype=str)\n",
    "\n",
    "# 3. í•œê¸€ ì»¬ëŸ¼ëª… ì„¤ì •\n",
    "new_columns = [\n",
    "    'í‰ê· ê°€ê²©(ì›)', 'ë²•ì¸ì½”ë“œ', 'ë²•ì¸ì´ë¦„', 'ìƒí’ˆ ëŒ€ë¶„ë¥˜ ì½”ë“œ', 'ìƒí’ˆ ëŒ€ë¶„ë¥˜ ì´ë¦„',\n",
    "    'ìƒí’ˆ ì¤‘ë¶„ë¥˜ ì½”ë“œ', 'ìƒí’ˆ ì¤‘ë¶„ë¥˜ ì´ë¦„', 'ìƒí’ˆ ì†Œë¶„ë¥˜ ì½”ë“œ', 'ìƒí’ˆ ì†Œë¶„ë¥˜ ì´ë¦„',\n",
    "    'ë“±ê¸‰ì½”ë“œ', 'ë“±ê¸‰ì´ë¦„', 'ìµœê³ ê°€(ì›)', 'ìµœì €ê°€(ì›)', 'í¬ì¥ì½”ë“œ', 'í¬ì¥ì´ë¦„',\n",
    "    'ì‚°ì§€ì½”ë“œ', 'ì‚°ì§€ì´ë¦„', 'í¬ê¸°ì½”ë“œ', 'í¬ê¸°ì´ë¦„', 'ì´ê°€ê²©(ì›)', 'ì—°ì›”ì¼',\n",
    "    'ë§¤ë§¤êµ¬ë¶„', 'ë‹¨ìœ„ì½”ë“œ', 'ë‹¨ìœ„(kg)', 'ë‹¨ìœ„ë¬¼ëŸ‰(kg)', 'ë‹¨ìœ„ì´ë¬¼ëŸ‰(kg)', 'ë„ë§¤ì‹œì¥ì½”ë“œ', 'ë„ë§¤ì‹œì¥ì´ë¦„'\n",
    "]\n",
    "\n",
    "if len(df.columns) != len(new_columns):\n",
    "    raise ValueError(\"ì»¬ëŸ¼ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "df.columns = new_columns\n",
    "\n",
    "# 4. ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜\n",
    "df['ì—°ì›”ì¼'] = pd.to_datetime(df['ì—°ì›”ì¼'], errors='coerce')\n",
    "\n",
    "# 5. ìˆ«ì ì»¬ëŸ¼ ë³€í™˜\n",
    "numeric_columns = ['í‰ê· ê°€ê²©(ì›)', 'ìµœê³ ê°€(ì›)', 'ìµœì €ê°€(ì›)', 'ì´ê°€ê²©(ì›)', 'ë‹¨ìœ„ë¬¼ëŸ‰(kg)', 'ë‹¨ìœ„ì´ë¬¼ëŸ‰(kg)']\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 6. í˜¼í•© íƒ€ì… ê²½ê³  ì»¬ëŸ¼ ì²˜ë¦¬: NaNì„ ë¹ˆ ë¬¸ìì—´ë¡œ, ëª¨ë‘ ë¬¸ìì—´í™”\n",
    "for col in ['í¬ì¥ì´ë¦„', 'ì‚°ì§€ì´ë¦„', 'í¬ê¸°ì´ë¦„']:\n",
    "    df[col] = df[col].fillna('').astype(str)\n",
    "\n",
    "# 7. CSV ì €ì¥ (ì™„ì „ ì •ë¦¬ëœ ìƒíƒœ)\n",
    "df.to_csv(output_path, index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0add8419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9524\\736215019.py:8: DtypeWarning: Columns (9,13,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_path, encoding='cp949')\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9524\\736215019.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.rename(columns={\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9524\\736215019.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['í‰ê· ë‹¨ê°€(ì›)'] = df_selected['ì´ê¸ˆì•¡(ì›)'] / df_selected['ì´ê±°ë˜ëŸ‰(kg)']\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9524\\736215019.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['í‰ê· ë‹¨ê°€(ì›)'] = df_selected['í‰ê· ë‹¨ê°€(ì›)'].replace([float('inf'), -float('inf')], None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "input_path = f\"{item_nm}/ìœ í†µê³µì‚¬_ë„ë§¤ì‹œì¥_{item_nm}_í•œê¸€ì»¬ëŸ¼ëª….csv\"\n",
    "output_path = f\"{item_nm}/ìœ í†µê³µì‚¬_{item_nm}_ìš”ì•½ë°ì´í„°.csv\"\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(input_path, encoding='cp949')\n",
    "\n",
    "selected_columns = ['ì—°ì›”ì¼', 'ìƒí’ˆ ì¤‘ë¶„ë¥˜ ì½”ë“œ', 'ìƒí’ˆ ì¤‘ë¶„ë¥˜ ì´ë¦„', 'ìƒí’ˆ ì†Œë¶„ë¥˜ ì½”ë“œ', 'ìƒí’ˆ ì†Œë¶„ë¥˜ ì´ë¦„',\n",
    "                    'ì´ê°€ê²©(ì›)', 'ë‹¨ìœ„ì´ë¬¼ëŸ‰(kg)', 'ì‚°ì§€ì½”ë“œ', 'ì‚°ì§€ì´ë¦„']\n",
    "df_selected = df[selected_columns]\n",
    "df_selected.rename(columns={\n",
    "    'ìƒí’ˆ ì¤‘ë¶„ë¥˜ ì½”ë“œ':'í’ˆëª©ì½”ë“œ',\n",
    "    'ìƒí’ˆ ì¤‘ë¶„ë¥˜ ì´ë¦„':'í’ˆëª©ëª…',\n",
    "    'ìƒí’ˆ ì†Œë¶„ë¥˜ ì½”ë“œ':'í’ˆì¢…ì½”ë“œ',\n",
    "    'ìƒí’ˆ ì†Œë¶„ë¥˜ ì´ë¦„':'í’ˆì¢…ëª…',\n",
    "    'ì´ê°€ê²©(ì›)':'ì´ê¸ˆì•¡(ì›)',\n",
    "    'ë‹¨ìœ„ì´ë¬¼ëŸ‰(kg)':'ì´ê±°ë˜ëŸ‰(kg)'\n",
    "}, inplace=True)\n",
    "\n",
    "df_selected['í‰ê· ë‹¨ê°€(ì›)'] = df_selected['ì´ê¸ˆì•¡(ì›)'] / df_selected['ì´ê±°ë˜ëŸ‰(kg)']\n",
    "df_selected['í‰ê· ë‹¨ê°€(ì›)'] = df_selected['í‰ê· ë‹¨ê°€(ì›)'].replace([float('inf'), -float('inf')], None)\n",
    "df_selected.insert(df_selected.columns.get_loc('ì´ê±°ë˜ëŸ‰(kg)') + 1, 'í‰ê· ë‹¨ê°€(ì›)', df_selected.pop('í‰ê· ë‹¨ê°€(ì›)'))\n",
    "\n",
    "# ìƒˆ CSVë¡œ ì €ì¥\n",
    "df_selected.to_csv(output_path, index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "793dd88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ì—°ì›”ì¼         0\n",
       "í’ˆëª©ì½”ë“œ        0\n",
       "í’ˆëª©ëª…         8\n",
       "í’ˆì¢…ì½”ë“œ        0\n",
       "í’ˆì¢…ëª…         7\n",
       "ì´ê¸ˆì•¡(ì›)      0\n",
       "ì´ê±°ë˜ëŸ‰(kg)    0\n",
       "í‰ê· ë‹¨ê°€(ì›)     0\n",
       "ì‚°ì§€ì½”ë“œ        0\n",
       "ì‚°ì§€ì´ë¦„        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{item_nm}/ìœ í†µê³µì‚¬_{item_nm}_ìš”ì•½ë°ì´í„°.csv\", encoding = 'cp949')\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d757a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •ì œëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ë°°ì¶”/ìœ í†µê³µì‚¬_ë°°ì¶”_ìš”ì•½ë°ì´í„°_ì •ì œ.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ\n",
    "df = pd.read_csv(f\"{item_nm}/ìœ í†µê³µì‚¬_{item_nm}_ìš”ì•½ë°ì´í„°.csv\", encoding='cp949')\n",
    "\n",
    "# ìƒí’ˆ ì¤‘ë¶„ë¥˜ ì´ë¦„ ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "df['í’ˆëª©ì½”ë“œ'] = df['í’ˆëª©ì½”ë“œ'].replace('', pd.NA).fillna(item_nm)\n",
    "df['í’ˆëª©ëª…'] = df['í’ˆëª©ëª…'].replace('', pd.NA).fillna(item_nm)\n",
    "df['í’ˆì¢…ì½”ë“œ'] = df['í’ˆì¢…ì½”ë“œ'].replace('', pd.NA).fillna(item_nm)\n",
    "df['í’ˆì¢…ëª…'] = df['í’ˆì¢…ëª…'].replace('', pd.NA).fillna(item_nm)\n",
    "\n",
    "# ì‚°ì§€ì½”ë“œ ì •ì œ í•¨ìˆ˜ (ë¬¸ì â†’ 0, ì•ìë¦¬ 0 ì œê±°, 6ìë¦¬ ë§ì¶¤)\n",
    "def clean_origin_code_backpad(code):\n",
    "    code_str = str(code).strip()\n",
    "    replaced = re.sub(r'\\D', '0', code_str)      # ë¬¸ì â†’ 0\n",
    "    stripped = replaced.lstrip('0')              # ì•ìª½ 0 ì œê±°\n",
    "    if not stripped:  # ëª¨ë‘ 0ì´ê±°ë‚˜ ì œê±° ê²°ê³¼ ì—†ìŒ\n",
    "        stripped = '0'\n",
    "    return stripped[:6].ljust(6, '0')            # ì• 6ìë¦¬ + ë’¤ 0 íŒ¨ë”©\n",
    "\n",
    "# ì ìš©\n",
    "df['ì‚°ì§€ì½”ë“œ'] = df['ì‚°ì§€ì½”ë“œ'].apply(clean_origin_code_backpad)\n",
    "\n",
    "# ì €ì¥\n",
    "output_path = f\"{item_nm}/ìœ í†µê³µì‚¬_{item_nm}_ìš”ì•½ë°ì´í„°_ì •ì œ.csv\"\n",
    "df.to_csv(output_path, index=False, encoding='cp949')\n",
    "\n",
    "print(f\"ì •ì œëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76c211d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ì—°ì›”ì¼         0\n",
       "í’ˆëª©ì½”ë“œ        0\n",
       "í’ˆëª©ëª…         0\n",
       "í’ˆì¢…ì½”ë“œ        0\n",
       "í’ˆì¢…ëª…         0\n",
       "ì´ê¸ˆì•¡(ì›)      0\n",
       "ì´ê±°ë˜ëŸ‰(kg)    0\n",
       "í‰ê· ë‹¨ê°€(ì›)     0\n",
       "ì‚°ì§€ì½”ë“œ        0\n",
       "ì‚°ì§€ì´ë¦„        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{item_nm}/ìœ í†µê³µì‚¬_{item_nm}_ìš”ì•½ë°ì´í„°_ì •ì œ.csv\", encoding = 'cp949')\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddb059fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "file_path = f\"{item_nm}/ìœ í†µê³µì‚¬_{item_nm}_ìš”ì•½ë°ì´í„°_ì •ì œ.csv\"\n",
    "\n",
    "# CSV íŒŒì¼ ì½ê¸° (ê²½ê³  ë°©ì§€ë¥¼ ìœ„í•´ low_memory=False ì‚¬ìš©)\n",
    "df = pd.read_csv(file_path, encoding=\"cp949\", low_memory=False)\n",
    "\n",
    "# ì‚°ì§€ì½”ë“œê°€ ë¬¸ìì—´ë¡œ ì²˜ë¦¬ë˜ë„ë¡ ë³€í™˜\n",
    "df['ì‚°ì§€ì½”ë“œ'] = df['ì‚°ì§€ì½”ë“œ'].fillna('').astype(str)\n",
    "\n",
    "# í•„í„°ë§ ì¡°ê±´: totprc ë˜ëŠ” unit_tot_qtyê°€ NaNì´ê±°ë‚˜ 0ì¸ ê²½ìš°\n",
    "pattern = r'^[^0-9]+$'\n",
    "\n",
    "condition = (\n",
    "    (df['ì‚°ì§€ì½”ë“œ'] == '') |\n",
    "    (df['ì‚°ì§€ì½”ë“œ'].str.strip() == \"0\") |\n",
    "    (df['ì‚°ì§€ì½”ë“œ'].str.strip() == \"0.0\")\n",
    "#     df['plor_cd'].str.match(pattern, na=False)) \n",
    "#     (df['plor_nm'].isna() | (df['plor_nm'] == 0))\n",
    ")\n",
    "\n",
    "# ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” í–‰ ì¶”ì¶œ\n",
    "df_filtered = df[condition]\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df_filtered.to_csv(f\"{item_nm}/ê²°ì¸¡ì¹˜_{item_nm}.csv\", index=False, encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92382234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "summary_path = f\"{item_nm}/ìœ í†µê³µì‚¬_{item_nm}_ìš”ì•½ë°ì´í„°_ì •ì œ.csv\"\n",
    "region_code_path = \"í‘œì¤€ì½”ë“œ/ì‚°ì§€ì½”ë“œ_ì§íŒœ.csv\"\n",
    "\n",
    "# 2. CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "summary_df = pd.read_csv(summary_path, encoding='cp949')\n",
    "region_df = pd.read_csv(region_code_path, encoding='cp949')\n",
    "\n",
    "# 3. ì‚°ì§€ì½”ë“œ ë²”ìœ„ íŒŒì‹± í•¨ìˆ˜\n",
    "def parse_code_range(code_str):\n",
    "    if \"~\" in code_str:\n",
    "        start, end = code_str.split(\"~\")\n",
    "    else:\n",
    "        start = end = code_str\n",
    "    return int(start), int(end)\n",
    "\n",
    "# 4. ì‚°ì§€ì½”ë“œ ë²”ìœ„ ìˆ«ì ì»¬ëŸ¼ ìƒì„±\n",
    "region_df[['start_code', 'end_code']] = region_df['ì‚°ì§€ì½”ë“œ'].apply(\n",
    "    lambda x: pd.Series(parse_code_range(x))\n",
    ")\n",
    "\n",
    "# 5. ì‚°ì§€ì½”ë“œ â†’ ì§íŒœì‚°ì§€ì½”ë“œ/ì´ë¦„ ë§¤í•‘ í…Œì´ë¸” í™•ì¥\n",
    "expanded_rows = []\n",
    "for _, row in region_df.iterrows():\n",
    "    for code in range(row['start_code'], row['end_code'] + 1):\n",
    "        expanded_rows.append({\n",
    "            'ì‚°ì§€ì½”ë“œ': code,\n",
    "            'ì§íŒœì‚°ì§€ì½”ë“œ': row['ì§íŒœì‚°ì§€ì½”ë“œ'],\n",
    "            'ì§íŒœì‚°ì§€ì´ë¦„': row['ì§íŒœì‚°ì§€ì´ë¦„']\n",
    "        })\n",
    "expanded_map_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# 6. ìš”ì•½ ë°ì´í„° ì‚°ì§€ì½”ë“œ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜\n",
    "summary_df['ì‚°ì§€ì½”ë“œ'] = pd.to_numeric(summary_df['ì‚°ì§€ì½”ë“œ'], errors='coerce').astype('Int64')\n",
    "summary_df = summary_df[summary_df['ì‚°ì§€ì½”ë“œ'].notna()].copy()\n",
    "summary_df['ì‚°ì§€ì½”ë“œ'] = summary_df['ì‚°ì§€ì½”ë“œ'].astype(int)\n",
    "\n",
    "# 7. ì§íŒœ ì»¬ëŸ¼ ì œê±° í›„ ë³‘í•©\n",
    "summary_df = summary_df.drop(columns=['ì§íŒœì‚°ì§€ì½”ë“œ', 'ì§íŒœì‚°ì§€ì´ë¦„'], errors='ignore')\n",
    "summary_df = pd.merge(summary_df, expanded_map_df, on='ì‚°ì§€ì½”ë“œ', how='left')\n",
    "\n",
    "# 8. ì‚°ì§€ì´ë¦„ì´ ëˆ„ë½ëœ ê²½ìš° ì§íŒœì‚°ì§€ì´ë¦„ìœ¼ë¡œ ë³´ì™„\n",
    "summary_df['ì‚°ì§€ì´ë¦„'] = summary_df['ì‚°ì§€ì´ë¦„'].fillna(summary_df['ì§íŒœì‚°ì§€ì´ë¦„'])\n",
    "\n",
    "\n",
    "# 9. ê²°ê³¼ ì €ì¥\n",
    "summary_df.to_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ì§íŒœì‚°ì§€ì •ë¦¬.csv\", index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5467f89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ì—°ì›”ì¼         0\n",
       "í’ˆëª©ì½”ë“œ        0\n",
       "í’ˆëª©ëª…         0\n",
       "í’ˆì¢…ì½”ë“œ        0\n",
       "í’ˆì¢…ëª…         0\n",
       "ì´ê¸ˆì•¡(ì›)      0\n",
       "ì´ê±°ë˜ëŸ‰(kg)    0\n",
       "í‰ê· ë‹¨ê°€(ì›)     0\n",
       "ì‚°ì§€ì½”ë“œ        0\n",
       "ì‚°ì§€ì´ë¦„        0\n",
       "ì§íŒœì‚°ì§€ì½”ë“œ      0\n",
       "ì§íŒœì‚°ì§€ì´ë¦„      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ì§íŒœì‚°ì§€ì •ë¦¬.csv\", encoding = 'cp949')\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0c425a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²°ì¸¡ì¹˜ í¬í•¨ í–‰ 0ê°œë¥¼ 'ë°°ì¶”/ë°°ì¶”_ê²°ì¸¡ì¹˜í¬í•¨í–‰.csv'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œì™€ ì¸ì½”ë”©ìœ¼ë¡œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ì§íŒœì‚°ì§€ì •ë¦¬.csv\", encoding='cp949')\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ” í–‰ë§Œ ì¶”ì¶œ\n",
    "df_na = df[df.isna().any(axis=1)]\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "output_path = f\"{item_nm}/{item_nm}_ê²°ì¸¡ì¹˜í¬í•¨í–‰.csv\"\n",
    "df_na.to_csv(output_path, index=False, encoding='cp949')\n",
    "\n",
    "print(f\"ê²°ì¸¡ì¹˜ í¬í•¨ í–‰ {len(df_na)}ê°œë¥¼ '{output_path}'ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c88d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ì§íŒœì‚°ì§€ì •ë¦¬.csv\", encoding='cp949')\n",
    "\n",
    "# ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "columns_to_drop = ['ì‚°ì§€ì½”ë“œ', 'ì‚°ì§€ì´ë¦„', 'ì§íŒœì‚°ì§€ì´ë¦„']\n",
    "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# ìƒˆë¡œ ì¶”ê°€í•  ì»¬ëŸ¼ë“¤ (ì´ˆê¸°ê°’ì€ None)\n",
    "new_columns = [\n",
    "    'íœ´ì¼ì—¬ë¶€', 'ëª…ì ˆì§€ìˆ˜', 'ì‘ê¸°ì •ë³´'\n",
    "]\n",
    "for col in new_columns:\n",
    "    df[col] = None\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df.to_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ì»¬ëŸ¼ì •ë¦¬ì™„ë£Œ.csv\", index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e46e1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "summary_path = f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ì»¬ëŸ¼ì •ë¦¬ì™„ë£Œ.csv\"\n",
    "mapping_path = \"ê¸°ìƒ/ì‚°ì§€ì½”ë“œ_ì§íŒœ_ê´€ì¸¡ì§€ì _ë§¤í•‘ì™„ë£Œ.csv\"\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_summary = pd.read_csv(summary_path, encoding='cp949')\n",
    "df_mapping = pd.read_csv(mapping_path, encoding='cp949')\n",
    "\n",
    "# ì¤‘ë³µ ì œê±°\n",
    "df_mapping = df_mapping.drop_duplicates(subset='ì§íŒœì‚°ì§€ì½”ë“œ')\n",
    "\n",
    "# 'ì§íŒœì‚°ì§€ì½”ë“œ' ê¸°ì¤€ìœ¼ë¡œ 'ê´€ì¸¡ì§€ì ' ì»¬ëŸ¼ ë³‘í•©\n",
    "df_merge = pd.merge(\n",
    "    df_summary,\n",
    "    df_mapping[['ì§íŒœì‚°ì§€ì½”ë“œ', 'ê´€ì¸¡ì§€ì ']],\n",
    "    on='ì§íŒœì‚°ì§€ì½”ë“œ',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 'ì‘ê¸°ì •ë³´' ì»¬ëŸ¼ ì˜¤ë¥¸ìª½ì— 'ê´€ì¸¡ì§€ì ' ì»¬ëŸ¼ ì‚½ì…\n",
    "if 'ì‘ê¸°ì •ë³´' in df_merge.columns:\n",
    "    insert_idx = df_merge.columns.get_loc('ì‘ê¸°ì •ë³´') + 1  # 'ì‘ê¸°ì •ë³´' ë‹¤ìŒ ìœ„ì¹˜\n",
    "    ê´€ì¸¡ì§€ì _series = df_merge.pop('ê´€ì¸¡ì§€ì ')\n",
    "    df_merge.insert(insert_idx, 'ê´€ì¸¡ì§€ì ', ê´€ì¸¡ì§€ì _series)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df_merge.to_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ê´€ì¸¡ì§€ì ì¶”ê°€.csv\", index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c43b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "crop_path = f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ê´€ì¸¡ì§€ì ì¶”ê°€.csv\"\n",
    "weather_path = \"ê¸°ìƒ/ê¸°ìƒì²­_ì¼ê¸°ìš”ì†Œ.csv\"\n",
    "output_path = f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ê¸°ìƒë³‘í•©ì™„ë£Œ.csv\"\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df_crop = pd.read_csv(crop_path, encoding='cp949')\n",
    "df_weather = pd.read_csv(weather_path, encoding='cp949')\n",
    "\n",
    "# ë‚ ì§œ ì²˜ë¦¬ â†’ YYYYMMDD ë¬¸ìì—´\n",
    "df_crop['ì—°ì›”ì¼'] = pd.to_datetime(df_crop['ì—°ì›”ì¼'], errors='coerce')\n",
    "df_crop['ë‚ ì§œ'] = df_crop['ì—°ì›”ì¼'].dt.strftime('%Y%m%d')\n",
    "\n",
    "df_weather['TM'] = pd.to_datetime(df_weather['TM'], errors='coerce')\n",
    "df_weather['ë‚ ì§œ'] = df_weather['TM'].dt.strftime('%Y%m%d')\n",
    "\n",
    "# ê´€ì¸¡ì§€ì /STN ëª¨ë‘ intë¡œ í†µì¼ í›„ str ë³€í™˜\n",
    "df_crop['ê´€ì¸¡ì§€ì '] = df_crop['ê´€ì¸¡ì§€ì '].astype('Int64').astype(str)\n",
    "df_weather['STN'] = df_weather['STN'].astype('Int64').astype(str)\n",
    "\n",
    "# ë³‘í•© í‚¤ ìƒì„±\n",
    "df_crop['merge_key'] = df_crop['ë‚ ì§œ'] + '_' + df_crop['ê´€ì¸¡ì§€ì ']\n",
    "df_weather['merge_key'] = df_weather['ë‚ ì§œ'] + '_' + df_weather['STN']\n",
    "\n",
    "# ë‚ ì”¨ ì»¬ëŸ¼ ë§¤í•‘\n",
    "weather_cols = {\n",
    "    'TA_AVG': 'ì¼í‰ê· ê¸°ì˜¨',\n",
    "    'TA_MAX': 'ìµœê³ ê¸°ì˜¨',\n",
    "    'TA_MIN': 'ìµœì €ê¸°ì˜¨',\n",
    "    'HM_AVG': 'í‰ê· ìƒëŒ€ìŠµë„',\n",
    "    'RN_DAY': 'ê°•ìˆ˜ëŸ‰(mm)',\n",
    "    'RN_60M_MAX': '1ì‹œê°„ìµœê³ ê°•ìˆ˜ëŸ‰(mm)'\n",
    "}\n",
    "\n",
    "# ë³‘í•©ìš© ë°ì´í„° ì¤€ë¹„\n",
    "df_weather_subset = df_weather[['merge_key'] + list(weather_cols.keys())].copy()\n",
    "df_weather_subset.rename(columns=weather_cols, inplace=True)\n",
    "\n",
    "# ë³‘í•© ìˆ˜í–‰\n",
    "df_merged = pd.merge(df_crop, df_weather_subset, on='merge_key', how='left')\n",
    "\n",
    "# ì •ë¦¬: ë³‘í•©í‚¤ ë° ì¤‘ê°„ ë‚ ì§œ ì»¬ëŸ¼ ì œê±°\n",
    "df_merged.drop(columns=['merge_key', 'ë‚ ì§œ'], inplace=True)\n",
    "\n",
    "# <NA> â†’ ë¹ˆ ë¬¸ìì—´, NaN â†’ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "df_merged.replace(\"<NA>\", \"\", inplace=True)\n",
    "df_merged.fillna(\"\", inplace=True)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df_merged.to_csv(output_path, index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a37ec1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ê¸°ìƒë³‘í•©ì™„ë£Œ.csv\", encoding='cp949')\n",
    "\n",
    "# ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸\n",
    "columns_to_drop = ['ê´€ì¸¡ì§€ì ']\n",
    "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df.to_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ê¸°ìƒ.csv\", index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1dc339b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ê¸°ìƒ.csv\", encoding='cp949')\n",
    "\n",
    "# df = df[~(df['ì§íŒœì‚°ì§€ì½”ë“œ'] == 2000)]\n",
    "\n",
    "# df.to_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ê¸°ìƒ_ìˆ˜ì…ì‚°ì œê±°.csv\", index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d179190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# df = pd.read_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ê¸°ìƒ.csv\", encoding='cp949')\n",
    "# df['ì—°ì›”ì¼'] = pd.to_datetime(df['ì—°ì›”ì¼'])\n",
    "\n",
    "# # ì´ìƒì¹˜ ì œê±° í•¨ìˆ˜\n",
    "# def remove_outliers_iqr(group, column):\n",
    "#     if len(group) < 4:\n",
    "#         return group  # ë„ˆë¬´ ì ìœ¼ë©´ ì œê±°í•˜ì§€ ì•ŠìŒ\n",
    "    \n",
    "#     q1 = group[column].quantile(0.25)\n",
    "#     q3 = group[column].quantile(0.75)\n",
    "#     iqr = q3 - q1\n",
    "#     lower = q1 - 1.5 * iqr\n",
    "#     upper = q3 + 1.5 * iqr\n",
    "#     return group[(group[column] >= lower) & (group[column] <= upper)]\n",
    "\n",
    "# # ì¼ë³„ë¡œ ê·¸ë£¹í•‘í•˜ì—¬ ì´ìƒì¹˜ ì œê±°\n",
    "# daily_filtered = df.groupby('ì—°ì›”ì¼', group_keys=False).apply(\n",
    "#     remove_outliers_iqr, column='í‰ê· ë‹¨ê°€(ì›)'\n",
    "# )\n",
    "\n",
    "# # ë‚ ì§œë³„ ì œê±° ê°œìˆ˜ ê³„ì‚°\n",
    "# original_counts = df.groupby('ì—°ì›”ì¼').size().rename('ì›ë˜ìˆ˜')\n",
    "# filtered_counts = daily_filtered.groupby('ì—°ì›”ì¼').size().rename('ì œê±°í›„ìˆ˜')\n",
    "# removal_stats = pd.concat([original_counts, filtered_counts], axis=1).fillna(0).astype(int)\n",
    "# removal_stats['ì œê±°ìˆ˜'] = removal_stats['ì›ë˜ìˆ˜'] - removal_stats['ì œê±°í›„ìˆ˜']\n",
    "\n",
    "\n",
    "# # ì´ ì œê±° ìˆ˜ ê³„ì‚°\n",
    "# total_removed = removal_stats['ì œê±°ìˆ˜'].sum()\n",
    "\n",
    "# # âœ… ì¶œë ¥\n",
    "# print(\"âœ… ì´ ì œê±°ëœ ì´ìƒì¹˜ ìˆ˜:\", total_removed)\n",
    "# print(\"\\nğŸ“… ë‚ ì§œë³„ ì œê±° ìˆ˜ :\")\n",
    "# print(removal_stats[['ì œê±°ìˆ˜']])\n",
    "\n",
    "# # ê²°ê³¼ CSV ì €ì¥\n",
    "# daily_filtered.to_csv(f\"{item_nm}/{item_nm}_ì´ìƒì¹˜ì œê±°_ì¼ë³„ê¸°ì¤€.csv\", index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4339bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ ì œê±°ëœ ì´ìƒì¹˜ ìˆ˜: 119926\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(f\"{item_nm}/{item_nm}ìš”ì•½ë°ì´í„°_ê¸°ìƒ.csv\", encoding='cp949')\n",
    "df['ì—°ì›”ì¼'] = pd.to_datetime(df['ì—°ì›”ì¼'])\n",
    "\n",
    "# ì£¼ì°¨(ì›”ìš”ì¼ ì‹œì‘) ì»¬ëŸ¼ ìƒì„±\n",
    "df['ì£¼ì°¨ì‹œì‘'] = df['ì—°ì›”ì¼'].apply(lambda d: d - pd.Timedelta(days=d.weekday()))\n",
    "df['ì£¼ì°¨ë'] = df['ì£¼ì°¨ì‹œì‘'] + pd.Timedelta(days=6)\n",
    "df['ì£¼ì°¨'] = df['ì£¼ì°¨ì‹œì‘'].dt.strftime('%Y-%m-%d') + '~' + df['ì£¼ì°¨ë'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# êµ­ì‚°/ìˆ˜ì…ì‚° ë¶„ë¦¬\n",
    "df_korean = df[df['ì§íŒœì‚°ì§€ì½”ë“œ'] != 2000].copy()\n",
    "df_imported = df[df['ì§íŒœì‚°ì§€ì½”ë“œ'] == 2000].copy()\n",
    "\n",
    "# ì´ìƒì¹˜ ì œê±° í•¨ìˆ˜\n",
    "def remove_outliers_iqr(group, column):\n",
    "    if len(group) < 4:\n",
    "        return group\n",
    "    q1 = group[column].quantile(0.25)\n",
    "    q3 = group[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return group[(group[column] >= lower) & (group[column] <= upper)]\n",
    "\n",
    "# êµ­ì‚°ì— ëŒ€í•´ì„œë§Œ ì´ìƒì¹˜ ì œê±°\n",
    "filtered_korean = (\n",
    "    df_korean.groupby('ì£¼ì°¨', group_keys=False)\n",
    "    .apply(remove_outliers_iqr, column='í‰ê· ë‹¨ê°€(ì›)')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_imported = df_imported.reset_index(drop=True)\n",
    "\n",
    "# ì œê±° ê°œìˆ˜ ê³„ì‚°\n",
    "original_counts = df_korean.groupby('ì£¼ì°¨').size().rename('ì›ë˜ìˆ˜')\n",
    "filtered_counts = filtered_korean.groupby('ì£¼ì°¨').size().rename('ì œê±°í›„ìˆ˜')\n",
    "removal_stats = pd.concat([original_counts, filtered_counts], axis=1).fillna(0).astype(int)\n",
    "removal_stats['ì œê±°ìˆ˜'] = removal_stats['ì›ë˜ìˆ˜'] - removal_stats['ì œê±°í›„ìˆ˜']\n",
    "total_removed = removal_stats['ì œê±°ìˆ˜'].sum()\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(\"âœ… ì´ ì œê±°ëœ ì´ìƒì¹˜ ìˆ˜:\", total_removed)\n",
    "\n",
    "# êµ­ì‚°(ì´ìƒì¹˜ ì œê±°ë¨) + ìˆ˜ì…ì‚° ê²°í•©\n",
    "weekly_filtered = pd.concat([filtered_korean, df_imported], ignore_index=True)\n",
    "\n",
    "# ì»¬ëŸ¼ ì •ë¦¬\n",
    "weekly_filtered = weekly_filtered[['ì£¼ì°¨'] + [col for col in weekly_filtered.columns if col != 'ì£¼ì°¨']]\n",
    "weekly_filtered = weekly_filtered.drop(columns=['ì£¼ì°¨ì‹œì‘', 'ì£¼ì°¨ë'], errors='ignore')\n",
    "\n",
    "# ì—°ì›”ì¼ ê¸°ì¤€ ì •ë ¬\n",
    "weekly_filtered = weekly_filtered.sort_values(by='ì—°ì›”ì¼').reset_index(drop=True)\n",
    "\n",
    "# ê²°ê³¼ CSV ì €ì¥\n",
    "weekly_filtered.to_csv(f\"{item_nm}/{item_nm}_ì´ìƒì¹˜ì œê±°_ì£¼ê°„ê¸°ì¤€.csv\", index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a9d3740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ì—°ì›”ì¼ì´ ëª¨ë‘ ì œê±°ëœ ë‚ ì§œ ìˆ˜: 0\n",
      "ğŸ“Œ ì—°ì›”ì¼ì´ ëª¨ë‘ ì œê±°ëœ ë‚ ì§œ ëª©ë¡: []\n",
      "ğŸ“† len<4 ë˜ëŠ” ë‚ ì§œê°€ ì œê±°ëœ ì£¼ì°¨ ìˆ˜: 0\n",
      "ğŸ“… í•´ë‹¹ ì£¼ì°¨ ëª©ë¡: []\n"
     ]
    }
   ],
   "source": [
    "small_group_weeks = df_korean.groupby('ì£¼ì°¨').filter(lambda g: len(g) < 4)['ì£¼ì°¨'].unique()\n",
    "\n",
    "# 2. 'ì—°ì›”ì¼' ë‹¨ìœ„ë¡œ ì œê±°ëœ ë‚ ì§œ íŒë³„\n",
    "original_dates = set(df_korean['ì—°ì›”ì¼'].unique())\n",
    "filtered_dates = set(filtered_korean['ì—°ì›”ì¼'].unique())\n",
    "removed_dates = original_dates - filtered_dates  # ì œê±°ë˜ì–´ ì•„ì˜ˆ ì‚¬ë¼ì§„ ë‚ ì§œ\n",
    "\n",
    "# 3. í•´ë‹¹ ë‚ ì§œê°€ ì–´ëŠ ì£¼ì°¨ì— ì†í–ˆëŠ”ì§€ ë§¤í•‘\n",
    "removed_rows = df_korean[df_korean['ì—°ì›”ì¼'].isin(removed_dates)]\n",
    "removed_rows = removed_rows[['ì—°ì›”ì¼', 'ì£¼ì°¨']].drop_duplicates()\n",
    "\n",
    "# 4. ì¡°ê±´ í†µí•©: \"ë¬¸ì œì„± ìˆëŠ” ë‚ ì§œ ë° ì£¼ì°¨\"\n",
    "problematic_dates = removed_rows['ì—°ì›”ì¼'].tolist()\n",
    "problematic_weeks = sorted(set(small_group_weeks).union(set(removed_rows['ì£¼ì°¨'])))\n",
    "\n",
    "# 5. ê²°ê³¼ ì¶œë ¥\n",
    "print(\"âš ï¸ ì—°ì›”ì¼ì´ ëª¨ë‘ ì œê±°ëœ ë‚ ì§œ ìˆ˜:\", len(problematic_dates))\n",
    "print(\"ğŸ“Œ ì—°ì›”ì¼ì´ ëª¨ë‘ ì œê±°ëœ ë‚ ì§œ ëª©ë¡:\", problematic_dates)\n",
    "print(\"ğŸ“† len<4 ë˜ëŠ” ë‚ ì§œê°€ ì œê±°ëœ ì£¼ì°¨ ìˆ˜:\", len(problematic_weeks))\n",
    "print(\"ğŸ“… í•´ë‹¹ ì£¼ì°¨ ëª©ë¡:\", problematic_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce77d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ì´ìƒì¹˜ ì œê±° í›„ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(f\"{item_nm}/{item_nm}_ì´ìƒì¹˜ì œê±°_ì£¼ê°„ê¸°ì¤€.csv\", encoding='cp949')\n",
    "\n",
    "# êµ­ì‚°ë§Œ ê¸°ì¤€ìœ¼ë¡œ ì£¼ê°„ í‰ê·  ë‹¨ê°€ ê³„ì‚° (df_korean ì—†ì´)\n",
    "weekly_avg = df[df['ì§íŒœì‚°ì§€ì½”ë“œ'] != 2000].groupby('ì£¼ì°¨')['í‰ê· ë‹¨ê°€(ì›)'].mean().rename('ì£¼ê°„í‰ê· ë‹¨ê°€(ì›)')\n",
    "\n",
    "# ë³‘í•©\n",
    "df = df.merge(weekly_avg, on='ì£¼ì°¨', how='left')\n",
    "\n",
    "# ë“±ê¸‰ ë¶€ì—¬\n",
    "df['ë“±ê¸‰ì´ë¦„'] = np.select(\n",
    "    condlist=[\n",
    "        (df['ì§íŒœì‚°ì§€ì½”ë“œ'] != 2000) & (df['í‰ê· ë‹¨ê°€(ì›)'] > df['ì£¼ê°„í‰ê· ë‹¨ê°€(ì›)'] * 1.3),\n",
    "        (df['ì§íŒœì‚°ì§€ì½”ë“œ'] != 2000) & (df['í‰ê· ë‹¨ê°€(ì›)'] > df['ì£¼ê°„í‰ê· ë‹¨ê°€(ì›)'] * 0.7) & (df['í‰ê· ë‹¨ê°€(ì›)'] <= df['ì£¼ê°„í‰ê· ë‹¨ê°€(ì›)'] * 1.3),\n",
    "        (df['ì§íŒœì‚°ì§€ì½”ë“œ'] != 2000) & (df['í‰ê· ë‹¨ê°€(ì›)'] <= df['ì£¼ê°„í‰ê· ë‹¨ê°€(ì›)'] * 0.7),\n",
    "        (df['ì§íŒœì‚°ì§€ì½”ë“œ'] == 2000)\n",
    "    ],\n",
    "    choicelist=['ê³ ', 'ì¤‘', 'ì €', 'ìˆ˜ì…ì‚°'],\n",
    "    default='ë“±ê¸‰ë¶ˆëª…'\n",
    ")\n",
    "\n",
    "# ì»¬ëŸ¼ ì •ë¦¬\n",
    "cols = list(df.columns)\n",
    "for col in ['ì£¼ê°„í‰ê· ë‹¨ê°€(ì›)', 'ë“±ê¸‰ì´ë¦„']:\n",
    "    if col in cols:\n",
    "        cols.remove(col)\n",
    "\n",
    "idx_price = cols.index('í‰ê· ë‹¨ê°€(ì›)') if 'í‰ê· ë‹¨ê°€(ì›)' in cols else - 1\n",
    "idx_total = cols.index('ì´ê¸ˆì•¡(ì›)') if 'ì´ê¸ˆì•¡(ì›)' in cols else - 1\n",
    "\n",
    "if idx_price != -1:\n",
    "    cols.insert(idx_price + 1, 'ì£¼ê°„í‰ê· ë‹¨ê°€(ì›)')\n",
    "if idx_total != -1:\n",
    "    cols.insert(idx_total, 'ë“±ê¸‰ì´ë¦„')\n",
    "\n",
    "df = df[cols]\n",
    "\n",
    "df.to_csv(f\"{item_nm}/{item_nm}_ì´ìƒì¹˜ì œê±°_ì£¼ê°„ê¸°ì¤€_ë“±ê¸‰ì´ë¦„.csv\", index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc4503d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë“±ê¸‰í¬í•¨ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(f\"{item_nm}/{item_nm}_ì´ìƒì¹˜ì œê±°_ì£¼ê°„ê¸°ì¤€_ë“±ê¸‰ì´ë¦„.csv\", encoding='cp949')\n",
    "\n",
    "# ë“±ê¸‰ì´ë¦„ â†’ ë“±ê¸‰ì½”ë“œ ë§¤í•‘\n",
    "grade_map = {\n",
    "    'ê³ ': 11,\n",
    "    'ì¤‘': 12,\n",
    "    'ì €': 13,\n",
    "    'ìˆ˜ì…ì‚°': 20,\n",
    "    'ë“±ê¸‰ë¶ˆëª…': 99\n",
    "}\n",
    "\n",
    "# ë“±ê¸‰ì½”ë“œ ì»¬ëŸ¼ ìƒì„±\n",
    "df['ë“±ê¸‰ì½”ë“œ'] = df['ë“±ê¸‰ì´ë¦„'].map(grade_map)\n",
    "\n",
    "# ë“±ê¸‰ì´ë¦„ ì•ì— ë“±ê¸‰ì½”ë“œ ì‚½ì…\n",
    "cols = list(df.columns)\n",
    "if 'ë“±ê¸‰ì´ë¦„' in cols and 'ë“±ê¸‰ì½”ë“œ' in cols:\n",
    "    cols.remove('ë“±ê¸‰ì½”ë“œ')\n",
    "    idx = cols.index('ë“±ê¸‰ì´ë¦„')\n",
    "    cols.insert(idx, 'ë“±ê¸‰ì½”ë“œ')\n",
    "    df = df[cols]\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "df.to_csv(f\"{item_nm}/{item_nm}_ì´ìƒì¹˜ì œê±°_ì£¼ê°„ê¸°ì¤€_ë“±ê¸‰ì½”ë“œ.csv\", index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07d24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. ë‘ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(f\"EDA/{item_nm}(EDAìš©)_ìŠ¤ì¼€ì¼ë§ë§Œ.csv\", encoding=\"cp949\")\n",
    "df_ext = pd.read_csv(\"í‘œì¤€ì½”ë“œ/factor_external_weekly.csv\", encoding=\"cp949\")\n",
    "\n",
    "columns_to_drop = [\"holiday_flag\", \"holiday_score\", \"grow_score\"]\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "# 2. factor_external_weeklyì—ì„œ weekno â†’ year, week ë¶„ë¦¬\n",
    "if 'weekno' in df_ext.columns:\n",
    "    df_ext['year'] = df_ext['weekno'].astype(str).str[:4].astype(int)\n",
    "    df_ext['week'] = df_ext['weekno'].astype(str).str[4:].astype(int)\n",
    "    df_ext.drop(columns=['weekno'], inplace=True)\n",
    "\n",
    "# 3. item_codeê°€ 1101ì¸ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "df_ext_filtered = df_ext[df_ext['item_code'] == item_cd]\n",
    "    \n",
    "# 4. ì¤‘ë³µ ì œê±° (year, week ê¸°ì¤€)\n",
    "df_ext_unique = df_ext_filtered.drop_duplicates(subset=['year', 'week'])\n",
    "\n",
    "# 5. ë³‘í•© (year, week ê¸°ì¤€)\n",
    "df_merged = pd.merge(\n",
    "    df,\n",
    "    df_ext_unique[['year', 'week', 'holiday_flag', 'holiday_score', 'grow_score']],\n",
    "    on=['year', 'week'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 6. 'í‰ê· ë‹¨ê°€(ì›)' ì™¼ìª½ì— ì‚½ì…\n",
    "price_col_index = df_merged.columns.get_loc('í‰ê· ë‹¨ê°€(ì›)')\n",
    "for col in ['grow_score', 'holiday_score', 'holiday_flag']:\n",
    "    if col in df_merged.columns:\n",
    "        df_merged.insert(price_col_index, col, df_merged.pop(col))\n",
    "\n",
    "# 7. ê²°ê³¼ ì €ì¥\n",
    "df_merged.to_csv(f\"EDA/{item_nm}(EDAìš©)_ìŠ¤ì¼€ì¼ë§ë§Œ_ë³‘í•©.csv\", index=False, encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4a9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-nlp",
   "language": "python",
   "name": "ml-dl-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
